{
 "metadata": {
  "name": "",
  "signature": "sha256:367fb90237f3968d8b40e86a880004a81287aa2360a5d77bde53e8598eabd730"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Original notebook is 00-fetch_data.ipynb in\n",
      "http://nbviewer.ipython.org/github/ioos/secoora/blob/master/notebooks/timeSeries/ssh/00-fetch_data.ipynb\n",
      "\"\"\"\n",
      "\n",
      "title = \"Using a CSW catalog to fetch Sea Surface Height data\"\n",
      "name = '2015-08-24-CSW_catalog'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import os\n",
      "from datetime import datetime\n",
      "from IPython.core.display import HTML\n",
      "\n",
      "# Metadata and markdown generation.\n",
      "hour = datetime.utcnow().strftime('%H:%M')\n",
      "comments = \"true\"\n",
      "\n",
      "date = '-'.join(name.split('-')[:3])\n",
      "slug = '-'.join(name.split('-')[3:])\n",
      "\n",
      "metadata = dict(title=title,\n",
      "                date=date,\n",
      "                hour=hour,\n",
      "                comments=comments,\n",
      "                slug=slug,\n",
      "                name=name)\n",
      "\n",
      "markdown = \"\"\"Title: {title}\n",
      "date:  {date} {hour}\n",
      "comments: {comments}\n",
      "slug: {slug}\n",
      "\n",
      "{{% notebook {name}.ipynb cells[2:] %}}\n",
      "\"\"\".format(**metadata)\n",
      "\n",
      "content = os.path.abspath(os.path.join(os.getcwd(), os.pardir,\n",
      "                                       os.pardir, '{}.md'.format(name)))\n",
      "\n",
      "with open('{}'.format(content), 'w') as f:\n",
      "    f.writelines(markdown)\n",
      "\n",
      "# Styles.\n",
      "with open('./styles/custom.css', 'r') as f:\n",
      "    styles = f.read()\n",
      "\n",
      "HTML(styles)\n",
      "\n",
      "html = \"\"\"\n",
      "<small>\n",
      "<p> This post was written as an IPython notebook.  It is available for\n",
      "<a href=\"http://ioos.github.com/system-test/downloads/\n",
      "notebooks/%s.ipynb\">download</a> or as a static\n",
      "<a href=\"http://nbviewer.ipython.org/url/ioos.github.com/\n",
      "system-test/downloads/notebooks/%s.ipynb\">html</a>.</p>\n",
      "<p></p>\n",
      "\"\"\" % (name, name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "\n",
      "start_time = time.time()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext watermark\n",
      "%watermark --githash --machine --python --packages iris,pyoos,owslib"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Save configuration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "try:\n",
      "    import cPickle as pickle\n",
      "except ImportError:\n",
      "    import pickle\n",
      "\n",
      "import iris\n",
      "from datetime import datetime\n",
      "from utilities import CF_names, fetch_range, start_log\n",
      "\n",
      "# 1-week start of data.\n",
      "kw = dict(start=datetime(2014, 7, 1, 12), days=6)\n",
      "start, stop = fetch_range(**kw)\n",
      "\n",
      "# SECOORA region (NC, SC GA, FL).\n",
      "bbox = [-87.40, 24.25, -74.70, 36.70]\n",
      "\n",
      "# CF-names.\n",
      "sos_name = 'water_surface_height_above_reference_datum'\n",
      "name_list = CF_names[sos_name]\n",
      "\n",
      "# Units.\n",
      "units = iris.unit.Unit('meters')\n",
      "\n",
      "# Logging.\n",
      "run_name = '{:%Y-%m-%d}'.format(stop)\n",
      "log = start_log(start, stop, bbox)\n",
      "\n",
      "# SECOORA models.\n",
      "secoora_models = ['SABGOM', 'USEAST',\n",
      "                  'USF_ROMS', 'USF_SWAN', 'USF_FVCOM']\n",
      "\n",
      "# Config.\n",
      "fname = os.path.join(run_name, 'config.pkl')\n",
      "config = dict(start=start,\n",
      "              stop=stop,\n",
      "              bbox=bbox,\n",
      "              name_list=name_list,\n",
      "              units=units,\n",
      "              run_name=run_name,\n",
      "              secoora_models=secoora_models)\n",
      "\n",
      "with open(fname,'wb') as f:\n",
      "    pickle.dump(config, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from owslib import fes\n",
      "from utilities import fes_date_filter\n",
      "\n",
      "kw = dict(wildCard='*',\n",
      "          escapeChar='\\\\',\n",
      "          singleChar='?',\n",
      "          propertyname='apiso:AnyText')\n",
      "\n",
      "or_filt = fes.Or([fes.PropertyIsLike(literal=('*%s*' % val), **kw)\n",
      "                  for val in name_list])\n",
      "\n",
      "# Exclude ROMS Averages and History files.\n",
      "not_filt = fes.Not([fes.PropertyIsLike(literal='*Averages*', **kw)])\n",
      "\n",
      "begin, end = fes_date_filter(start, stop)\n",
      "filter_list = [fes.And([fes.BBox(bbox), begin, end, or_filt, not_filt])]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from owslib.csw import CatalogueServiceWeb\n",
      "\n",
      "endpoint = 'http://www.ngdc.noaa.gov/geoportal/csw'\n",
      "csw = CatalogueServiceWeb(endpoint, timeout=60)\n",
      "csw.getrecords2(constraints=filter_list, maxrecords=1000, esn='full')\n",
      "\n",
      "fmt = '{:*^64}'.format\n",
      "log.info(fmt(' Catalog information '))\n",
      "log.info(\"URL: {}\".format(endpoint))\n",
      "log.info(\"CSW version: {}\".format(csw.version))\n",
      "log.info(\"Number of datasets available: {}\".format(len(csw.records.keys())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utilities import service_urls\n",
      "\n",
      "dap_urls = service_urls(csw.records, service='odp:url')\n",
      "sos_urls = service_urls(csw.records, service='sos:url')\n",
      "\n",
      "log.info(fmt(' CSW '))\n",
      "for rec, item in csw.records.items():\n",
      "    log.info('{}'.format(item.title))\n",
      "\n",
      "log.info(fmt(' SOS '))\n",
      "for url in sos_urls:\n",
      "    log.info('{}'.format(url))\n",
      "\n",
      "log.info(fmt(' DAP '))\n",
      "for url in dap_urls:\n",
      "    log.info('{}.html'.format(url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utilities import is_station\n",
      "\n",
      "# Filter out some station endpoints.\n",
      "non_stations = []\n",
      "for url in dap_urls:\n",
      "    try:\n",
      "        if not is_station(url):\n",
      "            non_stations.append(url)\n",
      "    except RuntimeError as e:\n",
      "        log.warn(\"Could not access URL {}. {!r}\".format(url, e))\n",
      "\n",
      "dap_urls = non_stations\n",
      "\n",
      "log.info(fmt(' Filtered DAP '))\n",
      "for url in dap_urls:\n",
      "    log.info('{}.html'.format(url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Add SECOORA models and observations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utilities import titles, fix_url\n",
      "\n",
      "for secoora_model in secoora_models:\n",
      "    if titles[secoora_model] not in dap_urls:\n",
      "        log.warning('{} not in the NGDC csw'.format(secoora_model))\n",
      "        dap_urls.append(titles[secoora_model])\n",
      "\n",
      "# NOTE: USEAST is not archived at the moment!\n",
      "# https://github.com/ioos/secoora/issues/173\n",
      "dap_urls = [fix_url(start, url) if\n",
      "            'SABGOM' in url else url for url in dap_urls]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import warnings\n",
      "from iris.exceptions import CoordinateNotFoundError, ConstraintMismatchError\n",
      "\n",
      "from utilities import (TimeoutException, secoora_buoys,\n",
      "                       quick_load_cubes, proc_cube)\n",
      "\n",
      "urls = list(secoora_buoys())\n",
      "if not urls:\n",
      "    raise ValueError(\"Did not find any SECOORA buoys!\")\n",
      "\n",
      "buoys = dict()\n",
      "for url in urls:\n",
      "    try:\n",
      "        with warnings.catch_warnings():\n",
      "            warnings.simplefilter(\"ignore\")  # Suppress iris warnings.\n",
      "            kw = dict(bbox=bbox, time=(start, stop), units=units)\n",
      "            cubes = quick_load_cubes(url, name_list)\n",
      "            cubes = [proc_cube(cube, **kw) for cube in cubes]\n",
      "        buoy = url.split('/')[-1].split('.nc')[0]\n",
      "        if len(cubes) == 1:\n",
      "            buoys.update({buoy: cubes[0]})\n",
      "        else:\n",
      "            #[buoys.update({'{}_{}'.format(buoy, k): cube}) for\n",
      "            # k, cube in list(enumerate(cubes))]\n",
      "            # FIXME: For now I am choosing the first sensor.\n",
      "            buoys.update({buoy: cubes[0]})\n",
      "    except (RuntimeError, ValueError, TimeoutException,\n",
      "            ConstraintMismatchError, CoordinateNotFoundError) as e:\n",
      "        log.warning('Cannot get cube for: {}\\n{}'.format(url, e))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyoos.collectors.coops.coops_sos import CoopsSos\n",
      "\n",
      "collector = CoopsSos()\n",
      "\n",
      "datum = 'NAVD'\n",
      "collector.set_datum(datum)\n",
      "\n",
      "collector.end_time = stop\n",
      "collector.start_time = start\n",
      "collector.variables = [sos_name]\n",
      "\n",
      "ofrs = collector.server.offerings\n",
      "title = collector.server.identification.title\n",
      "log.info(fmt(' Collector offerings '))\n",
      "log.info('{}: {} offerings'.format(title, len(ofrs)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import read_csv\n",
      "from utilities import sos_request\n",
      "\n",
      "params = dict(observedProperty=sos_name,\n",
      "              eventTime=start.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
      "              featureOfInterest='BBOX:{0},{1},{2},{3}'.format(*bbox),\n",
      "              offering='urn:ioos:network:NOAA.NOS.CO-OPS:WaterLevelActive')\n",
      "\n",
      "uri = 'http://opendap.co-ops.nos.noaa.gov/ioos-dif-sos/SOS'\n",
      "url = sos_request(uri, **params)\n",
      "observations = read_csv(url)\n",
      "\n",
      "log.info('SOS URL request: {}'.format(url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Clean the DataFrame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utilities import get_coops_metadata, to_html\n",
      "\n",
      "columns = {'datum_id': 'datum',\n",
      "           'sensor_id': 'sensor',\n",
      "           'station_id': 'station',\n",
      "           'latitude (degree)': 'lat',\n",
      "           'longitude (degree)': 'lon',\n",
      "           'vertical_position (m)': 'height',\n",
      "           'water_surface_height_above_reference_datum (m)': sos_name}\n",
      "\n",
      "observations.rename(columns=columns, inplace=True)\n",
      "\n",
      "observations['datum'] = [s.split(':')[-1] for s in observations['datum']]\n",
      "\n",
      "observations['sensor'] = [s.split(':')[-1] for s in observations['sensor']]\n",
      "observations['station'] = [s.split(':')[-1] for s in observations['station']]\n",
      "observations['name'] = [get_coops_metadata(s)[0] for s in observations['station']]\n",
      "\n",
      "observations.set_index('name', inplace=True)\n",
      "to_html(observations.head())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from utilities import secoora2df\n",
      "\n",
      "\n",
      "secoora_observations = secoora2df(buoys, varname='ssh above datum')\n",
      "to_html(secoora_observations.head())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import concat\n",
      "\n",
      "\n",
      "all_obs = concat([observations, secoora_observations], axis=0)\n",
      "to_html(concat([all_obs.head(2), all_obs.tail(2)]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Uniform 6-min time base for model/data comparison"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import DataFrame\n",
      "from owslib.ows import ExceptionReport\n",
      "from utilities import coops2df, save_timeseries\n",
      "\n",
      "iris.FUTURE.netcdf_promote = True\n",
      "\n",
      "log.info(fmt(' Observations '))\n",
      "outfile = '{}-OBS_DATA.nc'.format(run_name)\n",
      "outfile = os.path.join(run_name, outfile)\n",
      "\n",
      "log.info(fmt(' Downloading to file {} '.format(fname)))\n",
      "data, bad_datum = dict(), []\n",
      "for station in observations.station:\n",
      "    try:\n",
      "        df = coops2df(collector, station)\n",
      "        col = 'water_surface_height_above_reference_datum (m)'\n",
      "        data.update({station: df[col]})\n",
      "    except ExceptionReport as e:\n",
      "        bad_datum.append(station)\n",
      "        name = get_coops_metadata(station)[0]\n",
      "        log.warning(\"[{}] {}:\\n{}\".format(station, name, e))\n",
      "obs_data = DataFrame.from_dict(data)\n",
      "\n",
      "# Split good and bad vertical datum stations.\n",
      "pattern = '|'.join(bad_datum)\n",
      "if pattern:\n",
      "    all_obs['bad_datum'] = all_obs.station.str.contains(pattern)\n",
      "    observations = observations[~observations.station.str.contains(pattern)]\n",
      "\n",
      "# Save updated `all_obs.csv`.\n",
      "fname = '{}-all_obs.csv'.format(run_name)\n",
      "fname = os.path.join(run_name, fname)\n",
      "all_obs.to_csv(fname)\n",
      "\n",
      "comment = \"Several stations from http://opendap.co-ops.nos.noaa.gov\"\n",
      "kw = dict(longitude=observations.lon,\n",
      "          latitude=observations.lat,\n",
      "          station_attr=dict(cf_role=\"timeseries_id\"),\n",
      "          cube_attr=dict(featureType='timeSeries',\n",
      "                         Conventions='CF-1.6',\n",
      "                         standard_name_vocabulary='CF-1.6',\n",
      "                         cdm_data_type=\"Station\",\n",
      "                         comment=comment,\n",
      "                         datum=datum,\n",
      "                         url=url))\n",
      "\n",
      "save_timeseries(obs_data, outfile=outfile,\n",
      "                standard_name=sos_name, **kw)\n",
      "\n",
      "to_html(obs_data.head())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from pandas import DataFrame\n",
      "\n",
      "\n",
      "def extract_series(cube, station):\n",
      "    time = cube.coord(axis='T')\n",
      "    date_time = time.units.num2date(cube.coord(axis='T').points)\n",
      "    data = cube.data\n",
      "    return DataFrame(data, columns=[station], index=date_time)\n",
      "\n",
      "\n",
      "secoora_obs_data = []\n",
      "for station, cube in list(buoys.items()):\n",
      "    df = extract_series(cube, station)\n",
      "    secoora_obs_data.append(df)\n",
      "\n",
      "secoora_obs_data = concat(secoora_obs_data, axis=1)\n",
      "\n",
      "# These buoys need some QA/QC!\n",
      "mask = secoora_obs_data < -999\n",
      "secoora_obs_data[mask] = np.NaN\n",
      "\n",
      "# Interpolate to the same index as SOS.\n",
      "index = obs_data.index\n",
      "kw = dict(method='time', limit=30)\n",
      "secoora_obs_data = secoora_obs_data.reindex(index).interpolate(**kw).ix[index]\n",
      "\n",
      "log.info(fmt(' SECOORA Observations '))\n",
      "fname = '{}-SECOORA_OBS_DATA.nc'.format(run_name)\n",
      "fname = os.path.join(run_name, fname)\n",
      "\n",
      "log.info(fmt(' Downloading to file {} '.format(fname)))\n",
      "\n",
      "url = \"http://129.252.139.124/thredds/catalog_platforms.html\"\n",
      "comment = \"Several stations {}\".format(url)\n",
      "kw = dict(longitude=secoora_observations.lon,\n",
      "          latitude=secoora_observations.lat,\n",
      "          station_attr=dict(cf_role=\"timeseries_id\"),\n",
      "          cube_attr=dict(featureType='timeSeries',\n",
      "                         Conventions='CF-1.6',\n",
      "                         standard_name_vocabulary='CF-1.6',\n",
      "                         cdm_data_type=\"Station\",\n",
      "                         comment=comment,\n",
      "                         url=url))\n",
      "\n",
      "save_timeseries(secoora_obs_data, outfile=fname,\n",
      "                standard_name=sos_name, **kw)\n",
      "\n",
      "to_html(secoora_obs_data.head())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Loop discovered models and save the nearest time-series"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from iris.exceptions import (CoordinateNotFoundError, ConstraintMismatchError,\n",
      "                             MergeError)\n",
      "\n",
      "from utilities import time_limit, get_model_name, is_model, get_surface\n",
      "\n",
      "log.info(fmt(' Models '))\n",
      "cubes = dict()\n",
      "\n",
      "with warnings.catch_warnings():\n",
      "    warnings.simplefilter(\"ignore\")  # Suppress iris warnings.\n",
      "    for k, url in enumerate(dap_urls):\n",
      "        log.info('\\n[Reading url {}/{}]: {}'.format(k+1, len(dap_urls), url))\n",
      "        try:\n",
      "            with time_limit(60*5):\n",
      "                cube = quick_load_cubes(url, name_list, callback=None, strict=True)\n",
      "                if is_model(cube):\n",
      "                    cube = proc_cube(cube, bbox=bbox, time=(start, stop), units=units)\n",
      "                else:\n",
      "                    log.warning(\"[Not model data]: {}\".format(url))\n",
      "                    continue\n",
      "                cube = get_surface(cube)\n",
      "                mod_name, model_full_name = get_model_name(cube, url)\n",
      "                cubes.update({mod_name: cube})\n",
      "        except (TimeoutException, RuntimeError, ValueError,\n",
      "                ConstraintMismatchError, CoordinateNotFoundError,\n",
      "                IndexError) as e:\n",
      "            log.warning('Cannot get cube for: {}\\n{}'.format(url, e))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from iris.pandas import as_series\n",
      "\n",
      "from utilities import (make_tree, get_nearest_water,\n",
      "                       add_station, ensure_timeseries)\n",
      "\n",
      "for mod_name, cube in cubes.items():\n",
      "    fname = '{}-{}.nc'.format(run_name, mod_name)\n",
      "    fname = os.path.join(run_name, fname)\n",
      "    log.info(fmt(' Saving to file {} '.format(fname)))\n",
      "    try:\n",
      "        tree, lon, lat = make_tree(cube)\n",
      "    except CoordinateNotFoundError as e:\n",
      "        log.warning('Cannot create KDTree for: {}'.format(mod_name))\n",
      "        continue\n",
      "    # Get model series at observed locations.\n",
      "    raw_series = dict()\n",
      "    for station, obs in all_obs.iterrows():\n",
      "        try:\n",
      "            kw = dict(k=10, max_dist=0.04, min_var=0.01)\n",
      "            args = cube, tree, obs.lon, obs.lat\n",
      "            series, dist, idx = get_nearest_water(*args, **kw)\n",
      "        except ValueError as e:\n",
      "            status = \"No Data\"\n",
      "            log.info('[{}] {}'.format(status, obs.name))\n",
      "            continue\n",
      "        if not series:\n",
      "            status = \"Land   \"\n",
      "        else:\n",
      "            raw_series.update({obs['station']: series})\n",
      "            series = as_series(series)\n",
      "            status = \"Water  \"\n",
      "\n",
      "        log.info('[{}] {}'.format(status, obs.name))\n",
      "\n",
      "\n",
      "    if raw_series:  # Save cube.\n",
      "        for station, cube in raw_series.items():\n",
      "            cube = add_station(cube, station)\n",
      "        try:\n",
      "            cube = iris.cube.CubeList(raw_series.values()).merge_cube()\n",
      "        except MergeError as e:\n",
      "            log.warning(e)\n",
      "\n",
      "        ensure_timeseries(cube)\n",
      "        iris.save(cube, fname)\n",
      "        del cube\n",
      "\n",
      "    log.info('Finished processing [{}]: {}'.format(mod_name, url))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML(html)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}